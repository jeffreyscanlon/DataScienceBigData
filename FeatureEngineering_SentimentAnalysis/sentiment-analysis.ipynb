{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"sentiment-analysis.ipynb","provenance":[],"collapsed_sections":["EcYW9QtHsnwl","5ZAoJ_-6snyT","Va63pwOysnyu"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"dnBInVOSsntd"},"source":["# Sentiment analysis "]},{"cell_type":"code","metadata":{"id":"NpRKszXCsqVj","executionInfo":{"status":"ok","timestamp":1602974049811,"user_tz":240,"elapsed":362,"user":{"displayName":"Daniel Schnelbach","photoUrl":"","userId":"12875293149291665746"}}},"source":["### Jeff Scanlon\n","#### jscanlo2\n","\n","### Daniel Schnelbach\n","#### dschnelb"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5YxA7aoMsntg"},"source":["First parse and read in the file.  Each line is of the form:\n","\n","\n","0:\ttopic category label (books,\tcamera,\tdvd,\thealth,\tmusic,\tor\tsoftware)\t\n","1:\tsentiment\tcategory\tlabel\t(pos\tor\tneg)\t\n","2:\tdocument\tidentifier\t\n","3\tand\ton:\tthe\tdocument tokens\n","\n","We are only interested in 1 and 3.  Note that once a line is 'split' number 3 (above), the sentence is split into individual words.  These need to be 'joined' to form a sentence.\n"]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false},"id":"zHIelgpjsntj"},"source":["def read_file(fname='all_sentiment_shuffled.txt'):\n","    fp = open(fname, 'r', encoding='latin-1')\n","    all_labels = []\n","    all_text = []\n","    for line in fp:\n","        cat, lbl, _, *words = line.split() # '_' means don't care what it is\n","        all_labels.append(lbl)\n","        text = ' '.join(words) \n","        all_text.append(text)        \n","    return( all_labels, all_text )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VL6qgyLvsnt2","outputId":"924c82a0-f8be-49c4-bffd-64908fa30a33"},"source":["# Just so we know what join does\n","'-'.join(['list', 'of', 'strings'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'list-of-strings'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"6MyFCTgEsnuC"},"source":["As with the digits exercise, we return a list of labels and items to be classified in this case the sentences commenting on the product"]},{"cell_type":"code","metadata":{"id":"MAr5ug7-snuD"},"source":["all_labels, all_text = read_file()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NgAf-CvssnuN"},"source":["The below will only work on MacOS.  'wc'  (word count) is a unix program that counts the number of characters, words, and lines in a file.  With the command line argument is just counts the number of lines.  Note the '!' at the beginning of the file which tells the notebook that what follows is a shell command.  Note sure if these is a windows equivalent; windows users may have to comment this out. You can also check to see the number of entries by openign the file in an editor."]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false},"id":"lIUxWaBBsnuP","outputId":"5cbfda6b-da30-4577-8231-68675ed030ed"},"source":["!wc -l 'all_sentiment_shuffled.txt'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["11914 all_sentiment_shuffled.txt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vo3GIKWasnuY"},"source":["*Sanity check:*  have we read and processed the whole file: number of labels and sentences should equal the number of lines in the file."]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false},"id":"nrNlY48rsnuZ","outputId":"8147aace-2220-4bc3-ef01-ec70084238c7"},"source":["(len(all_labels), len(all_text))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(11914, 11914)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"_O2B9ycFsnul"},"source":["Check a few elements.  e.g., look at line 1"]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false},"id":"V9srFf3Wsnuo","outputId":"9d9d0ddc-5f8e-4dd8-87a5-fb4d9ef78898"},"source":["(all_labels[0], all_text[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('neg',\n"," \"i bought this album because i loved the title song . it 's such a great song , how bad can the rest of the album be , right ? well , the rest of the songs are just filler and are n't worth the money i paid for this . it 's either shameless bubblegum or oversentimentalized depressing tripe . kenny chesney is a popular artist and as a result he is in the cookie cutter category of the nashville music scene . he 's gotta pump out the albums so the record company can keep lining their pockets while the suckers out there keep buying this garbage to perpetuate more garbage coming out of that town . i 'll get down off my soapbox now . but country music really needs to get back to it 's roots and stop this pop nonsense . what country music really is and what it is considered to be by mainstream are two different things .\")"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"CLaExwRJsnuy"},"source":["## Bag of Words model"]},{"cell_type":"markdown","metadata":{"id":"VaoFKQUnsnu0"},"source":["The below URL that describes how to use `CountVectorizer`.  "]},{"cell_type":"markdown","metadata":{"id":"LpjZZ7ORsnu1"},"source":["http://scikit-learn.org/stable/modules/feature_extraction.html"]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false},"id":"muSw8rGhsnu3","outputId":"6cbc50aa-dea5-4408-b985-0c9ef532b065"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","vectorizer = CountVectorizer()\n","vectorizer"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CountVectorizer()"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"O7vXteJJsnvD"},"source":["corpus = [\n","     'This is the first document.',\n","     'This is the second second document.',\n","     'And the third one.',\n","     'Is this the first document?',\n"," ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false},"id":"b5ZMPVdbsnvN","outputId":"d4329ce0-9b03-4de6-80ff-9bb7d71bd410"},"source":["# how to use vectorizer?\n","\n","X = vectorizer.fit_transform(corpus)\n","\n","# Notice order is lost - a BAG is a kind of set.\n","vectorizer.get_feature_names()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false},"scrolled":false,"id":"SIus5STOsnvY","outputId":"3dd10037-02dc-40a9-9e0a-ab3e6bde3ce3"},"source":["X.toarray().shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4, 9)"]},"metadata":{"tags":[]},"execution_count":77}]},{"cell_type":"code","metadata":{"id":"Zi_91Qfmsnvj","outputId":"19f411ef-3426-4d49-f35c-b4419e037a1f"},"source":["X.toarray()[1][5] = X.toarray()[1][5] + 1\n","X.toarray()[1][5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"id":"umOzZqafsnvw","outputId":"98fbb486-534a-42fb-d1b7-d3e705f9dfa7"},"source":["import numpy as np\n","x = np.array([0,0,0,0])\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, 0])"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"code","metadata":{"id":"RvKmIdIYsnv5","outputId":"3d3b2886-fadd-4b1f-8533-aa85e7552741"},"source":["x[2] = x[2] + 1\n","x[2]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false},"id":"aH3BctDQsnwF","outputId":"f1bcc537-dd63-4ed3-ec76-222a47ca2760"},"source":["corpus[1]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'This is the second second document.'"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"daT0hfbvsnwP"},"source":["As described in the documentation at the URL, pass in all the text --- note that 'all_text' is an array of sentences, not an array of arrays of words.  When reading in the file the tokens should have been 'joined' to recreate the sentence."]},{"cell_type":"code","metadata":{"id":"oUF_DOfKsnwQ"},"source":["# We need to produce X_train\n","# how\n","\n","vectorizer = CountVectorizer()\n","X_train = vectorizer.fit_transform(all_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZQv_myEKsnwb"},"source":["A check to see how big the training matrix is.  It should have one entry for each sentence read in (11914).  The total number of unique words in the corpus turns out to be 46925"]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false},"id":"GBvUuQnasnwc","outputId":"e62b9e77-3cab-45c0-a491-9eaf23675ea9"},"source":["X_train.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(11914, 46933)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"EcYW9QtHsnwl"},"source":["### How many words (just words) are in the data set?"]},{"cell_type":"code","metadata":{"id":"tYYwMH7usnwn","outputId":"e7acb027-3ada-472c-a527-d458ad0d836b"},"source":["sum([len(text.split()) for text in all_text])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1794123"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"o3_fJSrQsnww"},"source":["If one wants, one can look at some of these features just to see what is going on.  Note that the vectorizer stores a sorted version of the features.  Hence the beginning of the feature list is a bunch of numbers.  It is only towards the middle that we see actual words. This is also discussed at the above URL."]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false},"id":"bRMnfHM8snwy","outputId":"f76a3f04-1465-4633-e540-35edbf2f2d09"},"source":["f = vectorizer.get_feature_names()\n","print(len(f))\n","print(f[:10])\n","print(f[2000:2010])\n","print(f[30000:30010])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["46933\n","['00', '000', '0003', '000mb', '004144', '007', '00am', '00pm', '01', '02']\n","['agendas', 'agent', 'agents', 'agentz', 'ager', 'agers', 'ages', 'agey', 'aggh', 'agglomerations']\n","['overpraised', 'overpriced', 'overproduced', 'overproduction', 'overpronnouncing', 'overprotecting', 'overrated', 'overrating', 'overreach', 'overreached']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ia9smJxfsnw7"},"source":["from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import BernoulliNB\n","from sklearn import metrics"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K4r_uFCHsnxD"},"source":["Feature vectors in this case indicate the count of words --- not real numbers.  Hence GausianNB should NOT be used. By our discussion in class, MultinomialNB should be used as we are dealing with discrete tokens.  BernoulliNB could also be used but we get better performance with Multinomial"]},{"cell_type":"code","metadata":{"id":"jmjGfU_1snxF"},"source":["model = MultinomialNB()\n","#model = BernoulliNB()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false},"id":"mGbkP5uhsnxM","outputId":"a1bd11be-1e72-43c2-fc91-608ddb891d21"},"source":["model.fit(X_train, all_labels)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultinomialNB()"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"V_Lr8EvqsnxW"},"source":["Another sanity check, we predict on the same set we training on.  Performance should be very high"]},{"cell_type":"code","metadata":{"id":"vNIr5AZxsnxY"},"source":["expected = all_labels\n","predicted = model.predict(X_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QptntpIQsnxf"},"source":["Did we get the correct number of predictions"]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false},"id":"deeYxSkFsnxg","outputId":"66bcd25b-85ee-432c-882c-6b4316f46774"},"source":["len(predicted)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["11914"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"lcewLOOgsnxq"},"source":["As one would expect, performance should be high --- we are testing on the training set!  If it isn't, then there is something not right in the above steps."]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false},"scrolled":true,"id":"oNQERBmjsnxr","outputId":"24da3cd7-175d-45cb-fcec-cf7871ef7d76"},"source":["print(metrics.accuracy_score(expected, predicted))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.9194225281181803\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZueLS9q2snxy"},"source":["Now, lets do the 10 fold cross validation"]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false},"id":"KNHogs9Msnxy"},"source":["from sklearn.model_selection import cross_val_score\n","y = all_labels\n","scores = cross_val_score(model, X_train, y, cv=10, scoring='accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p8OOXWbesnyA"},"source":["X_train and y are split into 10 folds.  `cross_val_score` automatically builds training sets with 9 of these folds and tests it against the remaining fold"]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false},"scrolled":true,"id":"1po0cXMMsnyC","outputId":"63d2146f-effe-4cc9-b13a-61d7ca13001d"},"source":["scores"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.8045302 , 0.80788591, 0.83892617, 0.81291946, 0.80856423,\n","       0.8186398 , 0.80268682, 0.79848866, 0.81612091, 0.81024349])"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"zCgANSmGsnyK"},"source":["Taking the mean of the score ..."]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false},"id":"7cuYgQrNsnyM","outputId":"b78eccd2-efd2-4d27-d329-13a0b0f7b10e"},"source":["scores.mean()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8119005657644864"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"if-aBHAnsnyS"},"source":["... we get pretty good performance"]},{"cell_type":"markdown","metadata":{"id":"5ZAoJ_-6snyT"},"source":["## Extensions\n","\n","We used a bigrams matrix instead of unigram matrix and returned the score boost below. See the scratchwork below - the functions could be used to ngrams of any type.."]},{"cell_type":"code","metadata":{"id":"3A0Y9hcjsnyT"},"source":["y = all_labels\n","scores = cross_val_score(model, x_train_2, y, cv=10, scoring='accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":false},"id":"M1Q9D0iHsnya"},"source":["scores = cross_val_score(model, x_train_2, y, cv=10, scoring='accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xkyo2ElAsnyg","outputId":"971295fa-135b-4bb2-a787-1df5f9ebff18"},"source":["scores"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.86073826, 0.85234899, 0.86661074, 0.85654362, 0.86146096,\n","       0.85390428, 0.85138539, 0.86397985, 0.87153652, 0.85894207])"]},"metadata":{"tags":[]},"execution_count":191}]},{"cell_type":"code","metadata":{"id":"IE-YRO_bsnyo","outputId":"864915bb-096a-4d3b-cc46-8f2621649b62"},"source":["scores.mean()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8597450678748331"]},"metadata":{"tags":[]},"execution_count":192}]},{"cell_type":"markdown","metadata":{"id":"Va63pwOysnyu"},"source":["## Get n-grams"]},{"cell_type":"code","metadata":{"id":"A9JXVAWXsnyv"},"source":["import re\n","\n","def generate_ngrams(string, n):\n","    \n","    x = re.sub(r'[^a-zA-Z0-9\\s]', ' ', string)\n","        \n","    tokens = [token for token in x.split(\" \") if token != \"\"]\n","    \n","    ngrams = zip(*[tokens[i:] for i in range(n)])\n","    \n","    return [\" \".join(ngram) for ngram in ngrams]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"5cxFcyqfsny0","outputId":"e5beee79-785e-415f-d941-ed52f3b1c056"},"source":["#Test\n","generate_ngrams(all_text[1], 2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['i was',\n"," 'was misled',\n"," 'misled and',\n"," 'and thought',\n"," 'thought i',\n"," 'i was',\n"," 'was buying',\n"," 'buying the',\n"," 'the entire',\n"," 'entire cd',\n"," 'cd and',\n"," 'and it',\n"," 'it contains',\n"," 'contains one',\n"," 'one song']"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"id":"6PVtSQxZsny6"},"source":["# Create ordered dict in which we make each n-gram a unique key and \n","# make its value correspond to its place in order (index)\n","\n","from collections import OrderedDict \n","d = OrderedDict()\n","idx = 0\n","for string in all_text:\n","    bigrams = generate_ngrams(string, 2)\n","    for b in bigrams:\n","        if b not in d.keys():\n","            d[b] = idx\n","            idx += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"YT6QaMulsny-","outputId":"908ea042-783f-4a69-d12a-8fd97045ca48"},"source":["# Just checking...\n","list(d.keys())[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['i bought',\n"," 'bought this',\n"," 'this album',\n"," 'album because',\n"," 'because i',\n"," 'i loved',\n"," 'loved the',\n"," 'the title',\n"," 'title song',\n"," 'song it']"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"code","metadata":{"scrolled":false,"id":"4Jaq25lbsnzC","outputId":"46d1adf4-df50-4e3d-c4fb-22568cae5b46"},"source":["list(d.values())[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"code","metadata":{"id":"uRvpsPNFsnzG","outputId":"4973f9b2-955b-46a4-f9ac-8a82f4de8fdc"},"source":["len(d.keys())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["512535"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"code","metadata":{"id":"0buMxQbMsnzK","outputId":"7ae9fb8f-d327-4bf7-c72b-3e6cd5fb63ad"},"source":["zero_arr = np.zeros((len(d.keys()),),dtype='int8')\n","zero_arr.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(512535,)"]},"metadata":{"tags":[]},"execution_count":163}]},{"cell_type":"code","metadata":{"id":"Al50HKgEsnzQ"},"source":["# Set array collector\n","arr_ls = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LoeVwnsLsnzU"},"source":["# For every string in review text, get the n-grams\n","# then iterate over each of those n-grams in the string and use the dict to get the index of the ZERO array\n","# we need to add a count to (+1)\n","# Append it all to a list or arrays. \n","for string in all_text:\n","    bigrams = generate_ngrams(string, 2)\n","    arr = np.zeros((len(d.keys()),),dtype='int8')\n","    for b in bigrams:\n","        arr[ d[b] ] = arr[ d[b] ] + 1\n","    arr_ls.append(arr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nset3tvCsnzY","outputId":"3f1d10a5-4118-469c-8279-b9396d807398"},"source":["len(arr_ls)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["11914"]},"metadata":{"tags":[]},"execution_count":158}]},{"cell_type":"code","metadata":{"id":"Coql_aFEsnzf","outputId":"8ec0781a-7343-4856-e822-855f46414c57"},"source":["arr_ls[3].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(512535,)"]},"metadata":{"tags":[]},"execution_count":159}]},{"cell_type":"code","metadata":{"id":"OMJl3XKEsnzj"},"source":["# These are all super inefficient and I should have looked for scipy sparse matrix functions earlier... \n","bigram_mx = np.vstack(arr_ls)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v4tkNfNVsnzm"},"source":["bigram_mx = bigram_mx.astype('int8')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4wLK8bDusnzr"},"source":["from scipy import sparse\n","x_train_2 = sparse.csr_matrix(bigram_mx) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RmQD2u6dsnzw","outputId":"8e5d4a31-d2d7-4a88-8742-9544efeb5bd7"},"source":["x_train_2"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<11914x512535 sparse matrix of type '<class 'numpy.int8'>'\n","\twith 1503144 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]},"execution_count":183}]},{"cell_type":"code","metadata":{"id":"3m_1F0Dhsnz3"},"source":["# get back memory...\n","del bigram_mx;\n","del arr_ls;"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JZ9po_xVsnz9"},"source":["## Got it... Back to the top to run. "],"execution_count":null,"outputs":[]}]}